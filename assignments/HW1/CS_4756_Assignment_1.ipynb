{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1gp0ZltmwYmpZ5uFqAVCXmg6iqk31sxwU",
      "authorship_tag": "ABX9TyMU8BcrjAqw59FkTsIIQlez",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/portal-cornell/cs4756_robot_learning/blob/main/assignments/HW1/CS_4756_Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Introduction**\n",
        "\n",
        "Welcome to your first official coding Assignment of 4756. In this short notebook, we are going to train agents to play the LunarLander game from OpenAI Gym, using data provided by an expert agent. Specifically, you will gain exposure to implementing behavioral cloning (BC) and dataset aggregation (DAgger) methods.\n",
        "\n",
        "**Evaluation:**\n",
        "Your code will be tested for correctness, and for certain assignments, speed. Please remember that all assignments should be completed individually.\n",
        "\n",
        "**Academic Integrity:** We will be checking your code against other submissions in the class for logical redundancy. If you copy someone else’s code and submit it with minor changes, we will know. These cheat detectors are quite hard to fool, so please don’t try. We trust you all to submit your own work only; please don’t let us down. If you do, we will pursue the strongest consequences available to us.\n",
        "\n",
        "**Getting Help:** The [#resources](https://www.cs.cornell.edu/courses/cs4756/2023sp/#resources) section on the course website is your friend (especially for this first assignment)! If you ever feel stuck in these projects, please feel free to avail yourself to office hours and Edstem! If you are unable to make any of the office hours listed, please let TAs know and we will be happy to assist. Since this is the first iteration of this course, please do not hesitate to reach out to TAs if you find any errors in the assignments. "
      ],
      "metadata": {
        "id": "2wHqE_y02_Xu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preliminaries\n",
        "\n",
        "In this assignment we will be using modules and agents from OpenAI Gym. Please run the following cells to make sure this notebook is properly configured. You should only need to run the following cell once, so feel free to comment it out after it has been installed the first time. \n",
        "\n",
        "If the `pip install` leaves you with any messages at the bottom telling you to install more packages, please feel free to add those lines in a separate cell. "
      ],
      "metadata": {
        "id": "HCdWyE4a5GPa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install gym[box2d]\n",
        "!pip3 install -q stable-baselines3[extra]"
      ],
      "metadata": {
        "id": "qYvcjjtZtFYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 1 imports\n",
        "from typing import Type, List\n",
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "\n",
        "# Part 2 imports\n",
        "from torch import optim\n",
        "\n",
        "# Part 3 imports\n",
        "import gym\n",
        "from stable_baselines3.ppo import PPO\n",
        "import torch.nn as nn\n",
        "import argparse"
      ],
      "metadata": {
        "id": "zfSwZBrW5EZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please run the following cells to get the expert data onto your noteboook. Then run `!ls` and verify that the file \"lunarlander_expert.zip\" exits\n",
        "\n"
      ],
      "metadata": {
        "id": "6avALbyl_lCK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -nc https://github.com/portal-cornell/cs4756_robot_learning/blob/main/assignments/HW1/LunarLander-v2/lunarlander_expert.zip?raw=true"
      ],
      "metadata": {
        "id": "Nwsnw2kL_zCB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mv lunarlander_expert.zip?raw=true lunarlander_expert.zip\n",
        "!ls | grep \"lunar\""
      ],
      "metadata": {
        "id": "JZR7D0YTAADN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To verify that everything has been downloaded correctly, please run the following cell and check for errors. If none appear, you're good to go! (You may ignore the warnings.warn() messages)"
      ],
      "metadata": {
        "id": "utIlHYCvtf-2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"LunarLander-v2\")\n",
        "if PPO.load(\"./lunarlander_expert\"):\n",
        "    print(\"Success!\")"
      ],
      "metadata": {
        "id": "IE45I_o8tfhd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Please make sure there are no errors in the above import statements before continuing to the rest of the assignment**"
      ],
      "metadata": {
        "id": "s_OmAcEK9ZVl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 1: Simple utilities for later use\n",
        "\n",
        "In the following section, we will define most of the helper methods that will become useful to you in training and evaluating you imitation agents. There are three sets of utility functions: \"NEURAL NET UTILS\", \"ENV UTILS\", and \"EVAL UTILS.\" Please take the time to understand what each function is doing, and also implement the `argmax_policy()` function in the second cell below. \n",
        "\n",
        "**Note: you may ignore all functions that deal with truncate until you get to the extra credit.**"
      ],
      "metadata": {
        "id": "UqGreDzW3UtL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== NEURAL NET UTILS ======\n",
        "\n",
        "def create_mlp(input_dim: int, output_dim: int, architecture: List[int], squash=False, activation: Type[nn.Module]=nn.ReLU) -> List[nn.Module]:\n",
        "    '''Creates a list of modules that define an MLP.'''\n",
        "    if len(architecture) > 0:\n",
        "        layers = [nn.Linear(input_dim, architecture[0]), activation()]\n",
        "    else:\n",
        "        layers = []\n",
        "        \n",
        "    for i in range(len(architecture) - 1):\n",
        "        layers.append(nn.Linear(architecture[i], architecture[i+1]))\n",
        "        layers.append(activation())\n",
        "    \n",
        "    if output_dim > 0:\n",
        "        last_dim = architecture[-1] if len(architecture) > 0 else input_dim\n",
        "        layers.append(nn.Linear(last_dim, output_dim))\n",
        "        \n",
        "    if squash:\n",
        "        # squashes output down to (-1, 1)\n",
        "        layers.append(nn.Tanh())\n",
        "    \n",
        "    return layers\n",
        "\n",
        "def create_net(input_dim: int, output_dim: int, squash=False):\n",
        "    layers = create_mlp(input_dim, output_dim, architecture=[64, 64], squash=squash)\n",
        "    net = nn.Sequential(*layers)\n",
        "    return net\n",
        "\n",
        "def argmax_policy(net):\n",
        "    # TODO: Return a FUNCTION that takes in a state, passes it through the network, and outputs index of the action with the highest probability.\n",
        "    # Inputs:\n",
        "    # - net: (type nn.Module). A neural network module, going from state dimension to number of actions.\n",
        "    # Wanted output:\n",
        "    # - argmax_fn: A function which takes in a state, and outputs argmax of the action vector.\n",
        "    pass\n",
        "\n",
        "def expert_policy(expert, s):\n",
        "    '''Returns a one-hot encoded action of what the expert predicts at state s.'''\n",
        "    action = expert.predict(s)[0]\n",
        "    one_hot_action = np.eye(4)[action]\n",
        "    return one_hot_action"
      ],
      "metadata": {
        "id": "xE0IeZIn4A-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== ENV UTILS ======\n",
        "\n",
        "def rollout(net, env, truncate=True):\n",
        "    '''Rolls out a trajectory in the environment, with optional state masking.'''\n",
        "    states = []\n",
        "    actions = []\n",
        "    \n",
        "    ob = env.reset()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "    \n",
        "    while not done:\n",
        "        states.append(ob.reshape(-1))\n",
        "        ob_tensor = torch.from_numpy(np.array(ob))\n",
        "        if truncate:\n",
        "            action = net(ob_tensor[:-2].float())\n",
        "        else:\n",
        "            action = net(ob_tensor.float())\n",
        "            \n",
        "        # detach action and convert to np array\n",
        "        if isinstance(action, torch.FloatTensor) or isinstance(action, torch.Tensor):\n",
        "            action = action.detach().numpy()\n",
        "        actions.append(action.reshape(-1))\n",
        "        \n",
        "        # step env\n",
        "        ob, r, done, _ = env.step(np.argmax(action))\n",
        "        total_reward += r\n",
        "        \n",
        "    states = np.array(states, dtype='float')\n",
        "    actions = np.array(actions, dtype='float')\n",
        "    return states, actions\n",
        "\n",
        "def expert_rollout(expert, env, truncate=False):\n",
        "    '''Rolls out an expert trajectory in the environment, with optional state masking.'''\n",
        "    expert_net = lambda s: expert.predict(s)[0]\n",
        "    return rollout(expert_net, env, truncate=truncate)"
      ],
      "metadata": {
        "id": "67LY5k2a4Hl3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== EVAL UTILS ======\n",
        "\n",
        "def eval_policy(policy, env, truncate=True):\n",
        "    '''Evaluates policy with one trajectory in environment. Returns accumulated reward.'''\n",
        "    done = False\n",
        "    ob = env.reset()\n",
        "    total_reward = 0\n",
        "    while not done:\n",
        "        if truncate:\n",
        "            action = policy(ob[:-2])\n",
        "        else:\n",
        "            action = policy(ob)\n",
        "        \n",
        "        # detach action and convert to np array\n",
        "        if isinstance(action, torch.FloatTensor) or isinstance(action, torch.Tensor):\n",
        "            action = action.detach().numpy()\n",
        "        \n",
        "        # step env and observe reward\n",
        "        ob, r, done, _ = env.step(action)\n",
        "        total_reward += r\n",
        "    \n",
        "    return total_reward"
      ],
      "metadata": {
        "id": "opfTF8cn4Kkb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 2: Behavioral Cloning & DAgger\n",
        "\n",
        "It is now time to build up our agents! Please read the directions carefully, and avail yourself to the myriad of resources in this class if you feel stuck!"
      ],
      "metadata": {
        "id": "G4jfPj8d4b7j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Behavioral cloning:** Behavioral cloning is the simplest imitation learning algorithm, where we perform supervised learning on the given (offline) expert dataset. We either do this via log likelihood maximization (cross entropy minimization) in the discrete action case, or mean-squared error minimization (can also do MLE) in the continuous control setting.\n",
        "\n",
        "Please implement the following `learn()` function for BC."
      ],
      "metadata": {
        "id": "aGj60deu47oV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BC:\n",
        "    def __init__(self, net, loss_fn):\n",
        "        self.net = net\n",
        "        self.loss_fn = loss_fn\n",
        "        \n",
        "        self.opt = optim.Adam(self.net.parameters(), lr=3e-4)\n",
        "        \n",
        "    def learn(self, env, states, actions, n_steps=1e4, truncate=True):\n",
        "        # TODO: Implement this method. Return the final greedy policy (argmax_policy).\n",
        "        pass"
      ],
      "metadata": {
        "id": "35nh46Hh4Mdd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset aggregation (DAgger):** DAgger is a fundamentally interactive algorithm, where we are able to query the expert any time we want to get information about how to proceed. This allows for significantly more freedom for the learner, as it can ask the expert anywhere and not be limited by the dataset that it is given to learn from.\n",
        "\n",
        "Like BC, please implement the following `learn()` function for DAgger."
      ],
      "metadata": {
        "id": "EhMweCoM5rkz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DAgger:\n",
        "    def __init__(self, net, loss_fn, expert):\n",
        "        self.net = net\n",
        "        self.loss_fn = loss_fn\n",
        "        self.expert = expert\n",
        "        \n",
        "        self.opt = optim.Adam(self.net.parameters(), lr=3e-4)\n",
        "        \n",
        "    def learn(self, env, n_steps=1e4, truncate=True):\n",
        "        # TODO: Implement this method. Return the final greedy policy (argmax_policy).\n",
        "        # Make sure you are making the learning process fundamentally expert-interactive.\n",
        "        pass"
      ],
      "metadata": {
        "id": "CBEGHL875stw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 3: Training loop\n",
        "\n",
        "Now with the hard part out of the way, it's time to see the performance of your networks! For imitation learning to work, all you need is access to some expert trajectories. The good news is, we've got you covered! 🙂\n",
        "\n",
        "Please implement the training loop under train() according to the instructions in the code."
      ],
      "metadata": {
        "id": "o_eQbXOp54dO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_env():\n",
        "    return gym.make(\"LunarLander-v2\")\n",
        "\n",
        "def get_expert():\n",
        "    return PPO.load(\"./lunarlander_expert.zip\")\n",
        "\n",
        "def get_expert_performance(env, expert):\n",
        "  Js = []\n",
        "  for _ in range(100):\n",
        "      obs = env.reset()\n",
        "      J = 0\n",
        "      done = False\n",
        "      hs = []\n",
        "      while not done:\n",
        "          action, _ = expert.predict(obs)\n",
        "          obs, reward, done, info = env.step(action)\n",
        "          hs.append(obs[1])\n",
        "          J += reward\n",
        "      Js.append(J)\n",
        "  ll_expert_performance = np.mean(Js)\n",
        "  return ll_expert_performance"
      ],
      "metadata": {
        "id": "QWjgmSRp7kTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(train_bc=True, truncate=False, n_steps=10000):\n",
        "    env = make_env()\n",
        "    expert = get_expert()\n",
        "    \n",
        "    performance = get_expert_performance(env, expert)\n",
        "    print('=' * 20)\n",
        "    print(f'Expert performance: {performance}')\n",
        "    print('=' * 20)\n",
        "    \n",
        "    # net + loss fn\n",
        "    if truncate:\n",
        "        net = create_net(input_dim=6, output_dim=4)\n",
        "    else:\n",
        "        net = create_net(input_dim=8, output_dim=4)\n",
        "    \n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    \n",
        "    if train_bc:\n",
        "        # TODO: train BC\n",
        "        # Things that need to be done:\n",
        "        # - Roll out the expert for X number of trajectories (a standard amount is 10).\n",
        "        # - Create our BC learner, and train BC on the collected trajectories.\n",
        "        # - It's up to you how you want to structure your data!\n",
        "        # - Evaluate the argmax_policy by printing the total rewards.\n",
        "        pass\n",
        "    else:\n",
        "        # TODO: train DAgger\n",
        "        # Things that need to be done.\n",
        "        # - Create our DAgger learner.\n",
        "        # - Set up the training loop. Make sure it is fundamentally interactive!\n",
        "        # - It's up to you how you want to structure your data!\n",
        "        # - Evaluate the argmax_policy by printing the total rewards.\n",
        "        pass"
      ],
      "metadata": {
        "id": "g4IzXF2cAzkA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_bc = True\n",
        "truncate = False\n",
        "n_steps = 10_000\n",
        "\n",
        "train(train_bc, truncate, n_steps)"
      ],
      "metadata": {
        "id": "gSgu22HQGM6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extra Credit:\n",
        "\n",
        "As a reminder, all extra credit problems are optional for students taking the 4xxx version of this course, but compulsory for students taking the 5xxx version.\n",
        "\n",
        "Using the `args.truncate` option, create a “partially observable” lunar lander environment where the angular velocity is masked out and not available to the learner (it’s still available to the expert!) You may find yourself needing to add/modify some of the existing code you have, so we recommend saving a version beforehand so that the performance of the fully-observable version is not affected by these changes.\n"
      ],
      "metadata": {
        "id": "7mOmEFGa6TQD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Writeup\n",
        "\n",
        "In a separate PDF, please include answers to the questions at the bottom of the Assignment [doc](https://docs.google.com/document/d/1YA8hpE7R8M0prgMtSMKHLdUzWZq3VSoW8mvMOw1EB90/edit#). Double check your work to make sure all cells are running properly. There are two additional answers for you to respond to in the writeup if you did the extra credit. \n",
        "\n",
        "**Some questions will ask for an accompanying graph to support your response. Don't forget to include these!**"
      ],
      "metadata": {
        "id": "RowlDUF06C86"
      }
    }
  ]
}
